We observed several noteworthy strategies participants took to learn about a package's social health.
First, it was important that they actually read text to assess social health.
They skimmed text from conversations to determine how welcoming communities were.
They read texts from documentation about a project's API and its mission and philosophy to determine if a package was designed for users like them.
Many tools for package comparisons show only summary statistics about a package (e.g.~\cite{awesome_python,ruby_toolbox}).
We think this seriously limits the helpfulness of these tools.

Participants queried the web through search engines to find pre-built summaries describing the community and documentation.
They consulted issue reports to determine how up-to-date documentation was.
They found explicit comparisons of packages on Reddit, and user testimonials on Quora.
% On these channels, the presence or absence of information can be an answer in itself.
When they looked at texts of questions, documents, and issue reports, they chose a handful of pages from dozens or even hundreds.
While their choices appeared to be informed at least partly by what appeared on the pages they were currently visiting, we hope future analysis will provide further insight into just how participants chose pages to visit.

% We are in the process of collecting developers' pathways through web documents when they answer social questions about packages.
Our URL log data suggests common places where participants find social health cues.
Figure~\ref{fig:visits} shows the location of cues for one developer answering three of the six questions.
Developers visited important communication channels~\cite{storey_revolution_2014}, including code hosting, web search, and Q\&A sites.
To summarize, we have seen participants find:
(1) whether a community is welcoming by viewing Q\&A sites and discussions on Reddit and Google Groups;
(2) documentation recency by viewing issue reports, code contribution histories, and pull request contents;
(3) the trustworthiness of developers by viewing issue reports, and profiles on code hosting sites.
We leave quantitative analysis of these trends and a full exploration of specific cues from these sites for future work.

Our preliminary results confirm a need for search tools that reveal obscure information about social health quickly.
Participants frequently realized they missed important information after twenty or thirty minutes of learning about a package.
This included wrongly assessing a package had no community at all, failing to find a newer version of the package under a different name, and missing a large repository of example code by the package's developers.
%searching that changes their perception of a package, often only after the right question is asked---for example, discovering that a package's primary maintainer has been inactive for two years.
Negative judgments could be costly to reverse.
One participant read source code before realizing that one package's programming API was preferable to one he favored before.

We believe these obstacles are a product of developers' habits and the limitations of current information interfaces.
With attention to both, we can design more powerful search tools to help developers quickly and effectively learn about packages' social health when choosing between them.
For now, these preliminary results suggest that
package consumers should not trust their initial instincts,
and should inspect comparisons, discussions, and Q\&A for a well-informed picture of a package's social health.
Developers should know that cues about the social health for their projects could exist on dozens of domains,
and potential consumers may need help learning where to look for help.

\if 0
Include the major insights that developers had during the study.
For example:
there's a new major version;
there's an examples repository that wasn't clearly shown;
there's a Google Groups page for the project;
the docs are actually pretty usable.
From this (and hopefully other insights) we see that a surface appearance of a package takes a while to build up accurately, and might require looking at a package from multiple different angles.
We think that it need not take so much time.

Not all developers seemed to agree on what information sources were best.
Part of this is likely because of prior knowledge.
One of the participants chose to look at the IRC channels for the package.

I want to touch upon each of the following.

What questions are participants least confident answering?

What are the most authoratative sources for answering each question?

What do they approximate to aggregate?

How much do participants' perceptions of documentation change?  And how much do their opinions change?

And what does this all suggest for what's broken for information presentation, and the design of programmers' front-end to the web?
\fi
