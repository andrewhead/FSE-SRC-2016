Developers work in an immense online network.
Developers benefit from this network by
leveraging social media and other channels to stay informed
and connect with other developers~\cite{singer_software_2014,storey_how_2016}.
Today there is a proliferation of socially-enabled channels~\cite{storey_revolution_2014} through which developers
answer each other's questions~\cite{mamykina_fastest_2011},
stay up-to-speed with rapidly changing software~\cite{linares-vasquez_how_2014},
help each other overcome bugs and learn new tools~\cite{parnin_blogging_2013},
and share information in many different forms at many different speeds.
The knowledge and conversations of programmers are increasingly distributed across the web.

Some community-developed tools aim to help developers compare packages.
Such community tools only show a fraction of the metrics used by developers.
Typical metrics for these tools (e.g.,~\cite{awesome_python,ruby_toolbox,package_quality}) tend to be based on counts and rates of code contributions, issue resolutions, downloads, and ``stars'' on code hosting sites like GitHub.
% Awesome Python\footnote{\url{https://python.libhunt.com/}} generates comparisons of arbitrary pairs of Python packages\footnote{\url{https://python.libhunt.com/project/pygame/vs/panda3}}, with an activity metrics based on commit frequency and an opaque numerical measurement of popularity.
% uby Toolbox\footnote{\url{https://www.ruby-toolbox.com/}} and the `packagequality' widget\footnote{\url{http://packagequality.com/}} also summarize package qualities with numerical ratings, where `packagequality' relies on issue response rates, download count, and version count.
% This isn't enough.
Developers may be concerned with
how much they respect the authors of code~\cite{robillard_field_2011},
how up-to-date the documentation is~\cite{storey_revolution_2014,nykaza_what_2002,lethbridge_how_2003,robillard_field_2011},
and whether the community is anti-social~\cite{storey_revolution_2014}.
We propose that carefully selected samples from communication channels can help developers make more informed judgments based on social health.
The current work presents a study to help us pick these samples.
% Developers will often have to make trade-offs between these factors when choosing software.
% In this abstract, we ask what these samples from developers' socially-enabled networks and web pages should be.

\if 0

Storey et al.~\cite{storey_revolution_2014}, in their survey of developers' use of media channels, present a list of digital channels, and digital channels enabled with social media, that we find to be representative of the channels that developers use for support.
In their survey, Storey et al.\ ask developers to name three channels that they find most important for development.
In our analysis, we focus on some of the top digital channels that developers mentioned the most, including code hosting, Q\&A, web search, and micro-blogging.
% We do not report non-digital channels, as from our review, these don't seem to have the same hazards of fragmented or missing information and anti-social behavior that was reported as being problematic for digital channels.

% The first three of these were explicitly mentioned in the three interviews we conducted with participants.
% We feel concentrating on these channels provides a diversity of channels, with different ways of communicating and finding information.

Developers face challenges when seeking support for software that they are reusing.
The right information is often missing in the place that developers are looking for it~\cite{robillard_field_2011,storey_revolution_2014,nykaza_what_2002}.
The right components may be accessible only with terms or concepts the developer isn't familiar with~\cite{nykaza_what_2002,jeong_improving_2009,robillard_field_2011} or require dependencies the developer doesn't know how to use.
Developers can find documentation untrustworthy~\cite{robillard_field_2011,storey_revolution_2014}, misleading, wrong, and out of date~\cite{nykaza_what_2002,lethbridge_how_2003,robillard_field_2011}.
The speed of answers on mailing lists and Q\&A sites can vary and take longer than developers want; slow response times can deter developers from asking questions or using a library.
Furthermore, information can be organized in a way that developers find costly to navigate~\cite{robillard_field_2011}, and can be fragmented in a way that requires developers to look in multiple places to find the information they need~\cite{jeong_improving_2009}.

There are already signals that programmers use to determine choices about trusting information and how to seek support online.
Some developers may rely on experience and intuition~\cite{storey_revolution_2014}.
They may inspect cosmetic features of web pages~\cite{brandt_two_2009}.
Authority and credibility of code examples can be assessed based on knowledge of and respect for the author of the code and evidence that the example is up to date~\cite{robillard_field_2011}.
% One of our interview participants mentioned that they are more likely to write bug reports to project authors that have a history or responsiveness (P2).
In short, there is evidence that surface-level features help in the moment when deciding whether to make use of online documents, and an understanding of project authors' background and history can help a developer decide what components to use and when to ask questions.

\fi

\if 0
During exploratory literature reviews, we encountered research in computer-supported cooperative work that offered interfaces to help readers assess the quality of socially curated content.
Examples of representations include: heat maps of readers' judgments of article credibility~\cite{pirolli_so_2009};
vignettes and text formatting revealing discussion and conflict around content snippets~\cite{towne_your_2013,borra_societal_2015};
visualizations of wiki contributions by user~\cite{arazy_recognizing_2010};
change request activity during development~\cite{begel_codebook_2010};
and groups of open source project commit messages, clustered by topic~\cite{hindle_whats_2009}.
From this small and very non-random sample, we have anecdotal evidence that there is research interest in providing front ends to help consumers of social content share opinions about credibility, assess authorship, and to coordinate teamwork.
\fi

\if 0
Programming is an incredibly information intensive activity.
What's more, for many programmers and software development activities, information seeking and code reuse is inseparable from programming.
As an example, consider the study by Brandt et al.~\cite{brandt_two_2009}.

Our work is motivated by an observation from a previous thread of research.
When interviewing a developer of programming documentation, we began to talk about ``badges.''
Badges are small indicators packaged in a standardized form that convey some single scalar dimension of the quality of the project:
for example, whether there is a chat channel for it, the number of stars on GitHub, and the percentage of code exercised by unit tests.

From personal experience, we knew that the story of finding support for a package is complex, rich, and immensely varied, even for a single package.
It would be comical to try to distill a sense of the entirety of support, strewn out over the web, into a very small space.
But we recognized that due to the developing norms of increasingly active online open source community, and the standard methods of access for this information (that is, using Google), fetching and interpreting a wide coverage of information for packages systematically is not out of the question.
So this is what we sought out to do.

This paper makes a mixed-methods contribution.
The primary enabler of this contribution is a system that, on-demand, captures longitudinal indicators of support that includes official documentation, Q\&A, and perhaps most uniquely, tutorial content.
To convey the complexity of finding packages with the right support, we conduct a handful of interviews with programmers from various backgrounds, revealing strategies, concerns, and pitfalls of working with online support for a package over a long lifetime of reuse.
To show that this data can be used in meaningful user-facing representations, we design a search user interface with the story of intending to help programmers select the right packages.
An evaluation in the lab shows that such representations, based on already-available (but currently distributed) data has the potential to change how programmers seek reusable programming components.
We leave the problem of wrapping this data for software developers as a method for ``software support telemetry'' as an item of future work for ourselves.

We believe that our techniques are applicable to creating user-facing indicators of online support for communities besides programmers.
This includes\ldots{}.
\fi
